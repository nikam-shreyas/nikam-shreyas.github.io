<!DOCTYPE html>
<html lang="en" data-bs-theme="dark">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Bokehlicious - Portrait Mode Effect</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN"
      crossorigin="anonymous"
    />
    <link rel="icon" href="../images/profile-pic-2.png" type="image/x-icon" />
    <link rel="stylesheet" href="../style.css" />
  </head>
  <body>
    <div
      class="container my-3 mx-auto parent-container"
      style="max-width: 47.5rem"
    >
      <!-- Navbar -->
      <nav class="navbar navbar-expand-lg">
        <div class="container-fluid">
          <a class="navbar-brand" href="../index.html"><b>SHREYAS NIKAM</b></a>
          <button
            class="navbar-toggler"
            type="button"
            data-bs-toggle="collapse"
            data-bs-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent"
            aria-expanded="false"
            aria-label="Toggle navigation"
          >
            <span class="navbar-toggler-icon"></span>
          </button>
          <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav me-auto mb-2 mb-lg-0">
              <li class="nav-item p-2">
                <a class="nav-link" aria-current="page" href="../about.html"
                  >About</a
                >
              </li>
              <li class="nav-item p-2">
                <a class="nav-link" href="../blog.html">Blog</a>
              </li>
              <li class="nav-item p-2 dropdown">
                <a
                  class="nav-link dropdown-toggle"
                  href="#"
                  id="navbarDropdownMenuLink"
                  role="button"
                  data-bs-toggle="dropdown"
                  aria-expanded="false"
                >
                  Projects
                </a>
                <ul
                  class="dropdown-menu"
                  aria-labelledby="navbarDropdownMenuLink"
                >
                  <li>
                    <a class="dropdown-item" href="../projects.html"
                      >Notable Projects</a
                    >
                  </li>
                  <li>
                    <a class="dropdown-item" href="projectideas.html"
                      >Project Ideas</a
                    >
                  </li>
                </ul>
              </li>
            </ul>
            <div class="d-flex">
              <!-- Change Theme Button -->
              <span
                role="button"
                class="fa-solid p-2 mt-1 fa-moon p-2"
                id="theme"
                onclick="changeTheme()"
                type="submit"
              ></span>
              <!-- Change Theme Button Ends -->
            </div>
          </div>
        </div>
      </nav>
      <!-- Navbar Ends -->

      <!-- Blog 2 starts -->
      <div>
        <p class="h2 mt-4">
          <!-- Title -->
          <a href="#" class="link"
            ><b
              >Bokeh-licious: Shallow Depth of Field for Images using Portrait
              Segmentation and Depth Estimation</b
            >
          </a>
          <!-- End Title -->
        </p>
        <!-- Details -->
        <p class="h5 mt-3 text-secondary">
          Dec 017, 2022 • 12 min • Computer Vision •
          <span
            class="btn btn-sm btn-outline-secondary text-muted brand-text-github"
            role="button"
            onclick="window.open('https://www.github.com/nikam-shreyas/Bokehlicious')"
          >
            <span class="fa-brands mt-1 fa-github"></span>
            Source
          </span>
        </p>

        <!-- End Details -->
        <!-- Description -->

        <p class="mt-4 text-muted">
          Have you ever admired those magazine-cover quality photos with the
          subject in crisp focus and a beautifully blurred background? The
          seemingly effortless way the subject pops from the image draws the eye
          and creates visual interest. Photographers achieve this by using
          shallow depth of field - minimizing the portion of the image that
          appears in focus. But mastering a shallow depth of field can be
          deceptively tricky, especially for smartphone cameras with tiny
          lenses. That's why in this project, I set out to develop an
          algorithmic approach to artificially generate this prized photographic
          effect.
        </p>
        <div class="text-center">
          <figure class="figure">
            <img
              src="../images/project3/results.gif"
              class="figure-img img-fluid rounded"
              alt="architecture"
            />
            <figcaption class="figure-caption text-center">
              Depth-based shallow depth of field effect. The further the object
              from the camera, the more blurred it is.
            </figcaption>
          </figure>
        </div>

        <p class="text-muted mt-4">
          By combining the powers of portrait segmentation and depth estimation
          - both enabled by deep learning - my method can take a standard photo
          and blur the background in a realistic, non-destructive manner. First,
          a deep neural network scans the image and detects the key subject,
          whether it's a person, pet or product. Next, a depth estimation model
          predicts the relative distance of objects in the scene. Finally, using
          the results from these models, my algorithm selectively applies blur
          to the background while keeping the subject crisp and clear. Think of
          it like a depth-aware digital paintbrush softly blurring some areas
          and sharpening others.
        </p>

        <p class="text-muted mt-4">
          While this novel approach sounds complex, I'll break it down
          step-by-step in this blog post so anyone can understand the concepts
          and code. I'll also showcase some before-and-after examples to
          highlight the visual impact. From amateurs to expert photographers, I
          think you'll be amazed by how well this technique emulates a shallow
          depth of field. So let's dive in and bring that professional bokeh
          look to any image!
        </p>
        <p class="h4 mt-4">Method</p>
        <p class="mt-2 text-muted">
          With the goal of artificially adding shallow depth of field, my
          approach relies on two main pillars - portrait segmentation and
          monocular depth estimation. Portrait segmentation allows us to
          identify and isolate the main subject, while depth estimation predicts
          the relative distance of objects in the image.
        </p>

        <figure class="figure">
          <img
            src="../images/project3/architecture.png"
            class="figure-img img-fluid rounded"
            alt="architecture"
          />
          <figcaption class="figure-caption text-center">
            Model Architecture. The full-sized image can be viewed

            <a
              href="https://drive.google.com/file/d/1DZC16KT3vt-M13xebAdKHjnFYlU1sosf/view?usp=share_link"
              class="text-info"
              >here</a
            >
          </figcaption>
        </figure>

        <p class="h4 mt-4">Datasets</p>
        <p class="mt-2 text-muted">
          A key ingredient of any deep learning system is the training data. For
          this project, I leverage two public datasets tailor-made for portrait
          segmentation and depth estimation respectively.
        </p>
        <ol class="text-muted mt-4">
          <li>
            For honing my portrait segmentation model, I utilize the PFCN
            dataset containing 1800 high resolution self-portraits from Flickr.
            This provides over 1000 examples of people as the main subjects,
            along with pixel-level masks outlining the person's shape.
          </li>
          <li>
            To teach my depth estimation model to understand object distances, I
            tap into the diverse NYU Depth dataset. Collected by NYU
            researchers, it consists of over 1000 aligned pairs of RGB images
            and corresponding depth maps. The depth maps encode object
            distances, allowing the model to learn the relationship between
            appearance and depth cues.
          </li>
        </ol>

        <p class="h4 mt-4"></p>

        <ol class="text-muted mt-2">
          <li>Load input image</li>
          <li>
            Compute its edge map using Sobel filter and append it as a 4th
            channel
          </li>
          <li>
            Pass this image to the portrait segmentation model to generate the
            portrait mask
          </li>
          <li>
            Pass the same image to the depth estimation model and generate the
            depth map
          </li>
          <li>
            Discretize the depth information into buckets such as 0 - 0.2, 0.2 -
            0.4 and so on.
          </li>
          <li>
            Apply a progressively stronger Gaussian blur to these buckets. This
            would result in the first bucket i.e. one containing pixels with
            depth in range 0 - 0.2 to be blurred less than the next bucket
            containing the pixels from the depth in range 0.2 - 0.4 and so on.
          </li>
          <li>
            Stack the pixels from all buckets together to form a single image.
          </li>
          <li>
            Superimpose the portrait mask onto the previous stacked output
            resulting in the final image with a rich bokeh.
          </li>
        </ol>

        <p class="h2 mt-4">Portrait Segmentation</p>

        <p class="mt-2 text-muted">
          Isolating the main subject is crucial for keeping them in sharp focus
          against a blurred background. To enable precise portrait extraction, I
          employ a U-Net architecture enhanced with DenseNet encoders and custom
          augmentations.
        </p>

        <p class="mt-2 text-muted">
          Starting with a vanilla U-Net, I found it struggled to fully segment
          portrait images after limited training. To boost its capabilities, I
          leveraged a DenseNet pre-trained on ImageNet as the encoder. This
          immediately improved results by initializing the model with useful
          visual features.
        </p>

        <p class="text-muted mt-2">
          However, some gaps still existed in the segmentation. I tackled this
          by providing an additional input to the network - the edge mask of the
          input image. Detecting strong edges on the portrait directs the
          model's attention to important shape contours.
        </p>
        <figure class="figure">
          <img
            src="../images/project3/output_mask.png"
            class="figure-img img-fluid rounded"
            alt="architecture"
          />
          <figcaption class="figure-caption text-center">
            Output masks across different architectures
          </figcaption>
        </figure>

        <p class="text-muted mt-2">
          This edge-aware approach, combined with longer training, enabled tight
          segmentation of subjects from challenging backgrounds. The model
          learns to rely on both textural cues from DenseNet and structural cues
          from the edges for robust extraction.
        </p>

        <p class="text-muted mt-2">
          After prediction, I further refine the masks using image processing
          techniques like morphological operations. This helps fill any residual
          holes and smooth the edges.
        </p>
        <div class="text-center">
          <figure class="figure">
            <img
              src="../images/project3/mask.png"
              class="figure-img img-fluid rounded"
              alt="architecture"
            />
            <figcaption class="figure-caption text-center">
              Border Mask
            </figcaption>
          </figure>
        </div>

        <p class="text-muted mt-2">
          The final precise masks allow isolated blurring of backgrounds without
          accidentally catching the subject. Segmentation accuracy is critical
          to realistically emulate depth of field effects.
        </p>

        <p class="h4 mt-4">Depth Estimation</p>
        <p class="text-muted mt-2">
          Similar to portrait segmentation, I attempted to achieve depth
          estimation with a U-Net architecture and a DenseNet encoder. However,
          the specified task turned out to be difficult for the model to achieve
          in the limited resources and time frame available.
        </p>

        <p class="text-muted mt-2">
          The outputs seemed to learn about the intensity values of the input
          instead of estimating the depth. I intuit the problem lies with the
          amount of data I chose for training the model in a limited resource
          environment (Google Colab). Studies have mentioned that they were
          required to train the model for at least 24 hours on multiple GPUs,
          which was not readily available to us.
        </p>

        <p class="text-muted mt-2">
          I compared the results of the outputs against the pre-trained model
          and finally resorted to using the pre-trained model to employ the
          progressive blur based on depth in the final processing pipeline
        </p>

        <figure class="figure">
          <img
            src="../images/project3/depth_estimation.png"
            class="figure-img img-fluid rounded"
            alt="architecture"
          />
          <figcaption class="figure-caption text-center">
            Depth estimates across the architectures
          </figcaption>
        </figure>

        <p class="h4 mt-4">Training Environment</p>

        <p class="text-muted mt-2">
          I trained the models on Google Colab. This allowed us to take
          advantage of the parallel computing capabilities of the their
          CUDA-enabled NVIDIA GPUs, which significantly reduced the training
          time compared to using a CPU-only machine. Overall, the results
          demonstrate the effectiveness of using a CUDA-enabled GPU for training
          deep-learning models on vision workloads and highlight the importance
          of considering hardware choices when optimizing training times.
        </p>

        <p class="h4 mt-4">Final result and conclusion:</p>

        <p class="text-muted mt-2">
          In conclusion, shallow depth of field can be achieved in portrait
          photography by using depth estimation and portrait segmentation
          techniques to identify the areas of the image that should be in focus
          and those that should be blurred. By applying a progressive blur to
          the image based on the estimated depth of each pixel, it is possible
          to create the illusion of shallow depth of field and draw the viewer's
          attention to the subject of the photograph. This technique can be
          useful for creating more dynamic and engaging portrait images.
        </p>

        <figure class="figure text-center">
          <img
            src="../images/project3/results.png"
            class="figure-img img-fluid rounded"
            alt="architecture"
          />
          <figcaption class="figure-caption text-center">
            Final Results
          </figcaption>
        </figure>

        <p class="h4 mt-4">Learnings:</p>

        <ul class="text-muted mt-4">
          <li>
            Successfully outperforming the baseline implementation for portrait
            segmentation underscored the importance of fine-tuning and extensive
            training for more complex tasks like depth estimation.
          </li>
          <li>
            Experimentally observing the impact of batch size adjustments on
            learning rates provided valuable insights into optimization
            strategies for neural network training.
          </li>
          <li>
            Implementing a progressive blur based on object depth revealed its
            potential in creating a natural-looking shallow depth of field
            effect, enhancing visual realism.
          </li>
          <li>
            Recognizing the pivotal role of batch normalization in stabilizing
            and aiding convergence in neural networks emphasized its
            significance in network training and performance improvement.
          </li>
          <li>
            Leveraging a pre-trained model in the encoder demonstrated superior
            performance compared to training from scratch, highlighting the
            efficiency and effectiveness of fine-tuning.
          </li>
          <li>
            Understanding the critical role of selecting the appropriate loss
            function in guiding the learning process proved essential for
            effective network training and task resolution.
          </li>
        </ul>

        <p class="text-muted mt-4">References:</p>
        <ol class="text-muted mt-2">
          <li>
            K. Purohit, M. Suin, P. Kandula, and R. Ambasamudram, "Depth-Guided
            Dense Dynamic Filtering Network for Bokeh Effect Rendering," 2019
            IEEE/CVF International Conference on Computer Vision Workshop
            (ICCVW), 2019, pp. 3417-3426, doi: 10.1109/ICCVW.2019.00424.
          </li>
          <li>
            Sergio Orts Escolano and Jana Ehman, "Accurate Alpha Matting for
            Portrait Mode Selfies on Pixel 6", Google AI Blog,
            <a
              href="https://ai.googleblog.com/2022/01/accurate-alpha-matting-for-portrait.html"
              >Link</a
            >
          </li>
          <li>
            Ignatov, A., Patel, J., and Timofte, R., “Rendering Natural Camera
            Bokeh Effect with Deep Learning”, arXiv e-prints, 2020.
          </li>
          <li>
            Li, Z., & Snavely, N. (2018). MegaDepth: Learning Single-View Depth
            Prediction from Internet Photos. 2018 IEEE/CVF Conference on
            Computer Vision and Pattern Recognition, 2041-2050.
          </li>
          <li>
            T. -Y. Kuo, Y. -C. Lo and Y. -Y. Lai, "Depth estimation from a
            monocular outdoor image," 2011 IEEE International Conference on
            Consumer Electronics (ICCE), 2011, pp. 161-162, doi:
            10.1109/ICCE.2011.5722517.
          </li>
          <li>
            Marcela Carvalho, Bertrand Le Saux, Pauline Trouvé-Peloux, Andrés
            Almansa, Fréderic Champagnat. On regression losses for deep depth
            estimation. ICIP 2018, Oct 2018, ATHENES, Greece.
            <a href="https://hal.archives-ouvertes.fr/hal-01925321">Link</a>
          </li>
          <li>
            Shen, X., Hertzmann, A., Jia, J., Paris, S., Price, B., Shechtman,
            E. and Sachs, I. (2016), Automatic Portrait Segmentation for Image
            Stylization. Computer Graphics Forum, 35: 93-102.
            <a href="https://doi.org/10.1111/cgf.12814">Link</a>
          </li>
          <li>
            <a
              href="https://www.kaggle.com/code/ajayvamsi123/portrait-segmentation-segnet/data"
              >Portrait Segmentation Dataset</a
            >
          </li>
          <li>
            <a href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html"
              >NYU Depth V2 Dataset</a
            >
          </li>
        </ol>
        <!-- End Description -->
      </div>
      <!-- Blog 2 Ends -->
      <hr class="mt-4" />
      <p class="h4 fw-bold">My other posts:</p>

      <div>
        <p class="h5 mt-3">
          <!-- Title -->
          <a href="blog-2-ctp.html" class="link"
            ><span class="h4">🌏</span> The power of data for good
          </a>
          <!-- End Title -->
        </p>
        <!-- Details -->
        <p class="h6 text-muted">Sep 15, 2023 • 2 min • Non-profit</p>
        <p class="h5 mt-4">
          <!-- Title -->
          <a href="blog-3-projectminusone.html" class="link"
            ><span class="h4">🌳</span> Project Minus One
          </a>
          <!-- End Title -->
        </p>
        <!-- Details -->
        <p class="h6 text-muted">June 15, 2018 • 2 min • Environment</p>
      </div>

      <!-- Footer -->
      <div>
        <div class="text-muted mb-5 pb-1">
          <hr />
          <span class="float-start">
            ©
            <a href="../index.html"> Shreyas Nikam</a>
          </span>
          <span class="float-end">
            <a href="https://www.linkedin.com/in/nikam-shreyas"
              ><span
                role="button"
                class="fa-brands fa-linkedin brand-text-linkedin fa-lg mx-1"
              ></span
            ></a>
            <a href="https://www.github.com/nikam-shreyas"
              ><span
                role="button"
                class="fa-brands fa-github brand-text-github fa-lg mx-1"
              ></span
            ></a>
            <a href="https://www.instagram.com/n.5hreyas/"
              ><span
                role="button"
                class="fa-brands fa-instagram brand-text-instagram fa-lg mx-1"
              ></span
            ></a>
          </span>
        </div>
      </div>
      <!-- Footer ends -->
    </div>

    <script src="../script.js"></script>
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://kit.fontawesome.com/db82bcb696.js"
      crossorigin="anonymous"
    ></script>
  </body>
</html>
